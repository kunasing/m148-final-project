# Appendix

## **i. Explain the exploratory data analysis that you conducted. What was done to visualize your data and split your data for training and testing?**

The features of the data include many spotify-created metrics such as danceability, acousticness, speechiness, instrumentalness, and liveness. It also contains information about the key, tempo, and time-signature, as well as the song’s popularity corresponding to how much the song has been played on Spotify. The feature we are most interested in is the genre, and we want to attempt to predict the genre based on the other data features. First we checked for missing and duplicate data and removed it. One of the things we discovered was that the data was capped at a 1000 values in each genre. So if we want to compare songs in different genres or have intra-genre comparisons, it would add more layers and make the comparison less valuable because of the limited number of songs in each genre. We decided to plot average popularity by genre to visualize it as a possible predictor variable. The code for this portion of the appendix did not change from project check-in 1\.

We visualized our data in a couple of different ways. The first thing we did was make a correlation heatmap of our features, which helped us visualize which numerical features are more positively and negatively correlated. We also wanted to explore how genre may be related to popularity, so we made a bar chart of genres by their average popularities. Finally, we plotted the distribution of all of our numerical features, to gauge what the numerical values looked like, and identify any patterns, outliers, or potential skewness in the data that could impact our analysis or model performance. All of these visualizations can be seen in the project check in.

Generally, to split our data from training and testing, we used sklearn’s train_test_split function, and split the data into 20% testing and 80% training. This was done throughout the project check ins for different models.

## **ii. What data pre-processing and feature engineering (or data augmentation) did you complete on your project?**

Our most important preprocessing feature was to eliminate duplicates and rows with missing values, as going into the project we expected to use all data features in our predictions. We found that there was only one row with any missing values among 114,000. Additionally, this was missing data in many of the features including the ones we were predicting and our predictors, so we simply removed this data point. After this, our other consideration was duplicate data points; to prevent skewing our data with these additions, we removed an additional 450 data points. Many of the duplicates were within a few genres including romance and classical, but we decided against doing data augmentation to create replacement data points to reach the threshold of 1,000 songs per genre because the data set was already large with a diverse number of features, and each genre still had at least around 850 out of the expected 1,000 songs, with most remaining at over 990\.

Another piece of data augmentation we did was principal component analysis. We initially used principal component analysis to help us do clustering, which primarily helped us observe outliers based on the durations of tracks due to PCA’s outlier sensitivity. Because of these results, we decided to also remove outliers that could affect our PCA and our end results. These outliers mostly consisted of extremely long songs (e.g entire soundtracks), songs in non-standard time signatures, and very “speechy” or “loud” songs that may have been significantly expecting outputs.  
After removing the outliers, we still had around 110,000 data points. Then, PCA helped us analyze the causes of our variance more accurately and improved the accuracy of our neural networks as opposed to using the unchanged features.

## **iii. How was regression analysis applied in your project? What did you learn about your data set from this analysis and were you able to use this analysis for feature importance? Was regularization needed?**

In linear regression analysis, we attempted to predict popularity. Our model was underfitting with an extremely low R-squared but we still used ridge regression with cross validation to help us learn which variables were more correlated. Doing linear regression helped us determine that for most of our project we would likely need to use more complex models in order to better understand and predict the dataset, but wasn’t able to help us much in terms of learning about the data features. If we attempted to predict another variable using linear regression we may have had better results, however many of those variables had a known collinearity where we hoped to predict something not intrinsic to the song itself. The skew of most songs having a low popularity alongside few songs with high popularity meant that regression was a factor in underfitting and popularity is also affected by dozens of external factors including social media, promotion, and artist fan bases. Ultimately, this was very unsuccessful as the high variance of popularity cannot realistically be modeled with a linear function, otherwise artists would simply create the most popular song based on the algorithm’s highest output. The code for this portion of the appendix did not change from project check-in 2\.

## **iv. How was logistic regression analysis applied in your project? What did you learn about your data set from this analysis and were you able to use this analysis for feature importance? Was regularization needed?**

Logistic regression didn’t have an obvious application in our project, as we had no binary variables. To apply logistic regression, we attempted to classify songs as fast paced or not fast paced based on their tempo (greater than 110 being fast paced). We learned that while logistic regression provided some insights into the relationship between features and tempo categories, the predictions were not highly accurate (mean accuracy of 65%), likely due to overlap between the features for fast and slow-paced songs. The model especially struggled with classifying negative points, with a true negative rate of 17%. We were not able to use this analysis for feature importance, as the model struggled to generalize. We did not need regularization because we didn’t observe any significant multicollinearity. The code for this portion of the appendix did not change from project check-in 3\.

## **v. How were KNN, decision trees, or random forest used for classification on your data? What method worked best for your data and why was it good for the problem you were addressing?**

We decided to apply the Random Forest (RF) Algorithm over K-Nearest Neighbors (KNN) due to the size and high-dimensionality of our dataset. KNN, while simple and interpretable, struggles with high-dimensional data and is computationally expensive with large datasets. Additionally, we opted for Random Forest to mitigate the risk of overfitting, which is common in single decision trees, especially when they grow deep without proper constraints. We used the RF Algorithm to determine the feature importance of 6 variables: popularity, energy, danceability, valence, loudness, and speechiness in predicting target variable: is_fast_paced. There was no binary variable that we believed to be satisfactory as a target variable in our dataset, so we came up with our own based on an existing column. We classified songs as “fast paced” based on their tempo- a song with a tempo greater than 110 is “fast paced”. The six independent variables were selected based on their correlation with tempo, as they represented the six highest correlations with the target variable. The default threshold of 0.5 had to be adjusted due to a class imbalance. We used threshold 0.65 since it reduces false positives and improves the overall precision of the positive class. The AUC of 74.07% indicates that the model had a strong ability to distinguish between fast paced and non-fast paced songs. Based on the ROC curve, we chose a prediction threshold of 0.4, since that point had the maximum difference between the ROC curve and the random guess curve. The code for this portion of the appendix did not change from project check-in 4\.

## **vi. How were PCA and clustering applied on your data? What method worked best for your data and why was it good for the problem you were addressing?**

There are 14 predictor variables available to us, but since we wanted to eventually perform clustering, using points in ℝ14 could lead to points being highly spread out, and thus yielding clusters that might not really share any similarities at all. So, we used Principal Component Analysis (PCA) to perform dimensionality reduction, which can help decrease computational complexity, as well as improve clustering performance by possibly eliminating the curse of dimensionality. The 'elbow' for the scree plot occurred very early (at 2 components), so we looked at the cumulative explained variability curve and determined the number of significant components by finding the point at which we reach 95% explained variability. Based on the plot, we passed the 95% variability threshold at around 11 principal components, so that is the number we proceeded with and resulted in the dimension-reduced dataset we used for clustering. We found the optimal number of clusters k by finding the 'elbow'-point in the graph of Within-Cluster Sum of Squares (WCSS) vs No. of clusters for k \= 1, 2, …, 15\. Our WCSS curve was quite smooth, but we identified an elbow at k \= 4\. There was clearly an imbalance in the clusters, so we explored why cluster 3 had so few data points compared to the others. We found that all metrics across clusters are relatively similar except \`duration_ms\`, which is where cluster 3's values far exceed the others. Our guess is that all the songs that have such a high duration formed their own cluster, as a result of being so spread out from the other data points. If we had wanted to analyze our dataset further without it being affected by these super-long songs, we could replace all \`duration_ms\` values above a certain threshold with a chosen value, like the mean of the remaining, or simply eliminate outliers from consideration. For our key method, we ended up deciding to remove outliers to improve our PCA which helped improve the accuracy of our predictions. The code for this portion of the appendix did not change from project check-in 5\.

## **vii. Explain how your project attempted to use a neural network on the data and the results of that attempt.**

This is discussed as our key method for genre classification in Part A. The updated code for our key method is attached below.

## **viii. Give examples of hyperparameter tuning that you applied in preparing your project and how you chose the best parameters for models.**

For tuning hyperparameters, we used a variety of rates: .01, .005, .001, .0005, .0001. We started with the higher learning rates and compared the loss curves for both training and validation across each iteration (300 total epochs). Across all learning rates, we had a downward trend in loss, but around 50 iterations the rate tended to begin oscillating while maintaining a shallow downward trend. We ended up settling on the learning rate of around 0.0005 because it resulted in the best loss reduction across our testing and validation sets.

Across this, we also tried using different optimizers as well as including techniques such as dropout and regularization (weight decay in this case), but as our issue was primarily underfitting the regularization parameter likely did not have a significant effect on our work. In the end, the accuracy of our predictions for both popularity prediction and genre classification remained relatively low despite our efforts. It is likely that without the computational limitations from using google colab we may have been able to further reduce loss after a much higher number of iterations to further improve both loss and accuracy.

# Dataset

Maharshipandya. “Spotify Track Dataset.” Spotify. https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rqfpO92Nefm"
   },
   "source": [
    "# WEEK 10 - Neural Networks\n",
    "## Iowa Platypuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yomU_Y9Y1qCB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "from re import X\n",
    "\n",
    "# utilize GPU for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W94yGPt8f16"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHge17e2XPD"
   },
   "source": [
    "### Upload and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TX4U4n7EAHu"
   },
   "source": [
    "In this section, we upload our raw data, and remove the only row that has missing values, at index 65900, as well as deleting duplicates which skew data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaitIaOEDBdl"
   },
   "outputs": [],
   "source": [
    "def clean_data (data):\n",
    "  rows_with_missing = data_raw.isnull().any(axis=1)\n",
    "  data_no_nans = data_raw.drop(index=65900)\n",
    "  data = data_no_nans.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKf64O8Al8xJ",
    "outputId": "1249fdf0-a658-49e8-8f47-a0f00b2281e5"
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"dataset.csv\", index_col=0)\n",
    "print(data_raw.shape)\n",
    "data = clean_data(data_raw)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WyYoqUaVgmB"
   },
   "source": [
    "### Remove Numerical Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRaLVJxNES4R"
   },
   "source": [
    "Numerical outliers can strongly affect the PCA which we will use for our neural networks, so we want to remove outliers to ensure that we can accurately predict values. Many of these songs have unique features that make genre categorization difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnajGDaF1J-g"
   },
   "outputs": [],
   "source": [
    "def get_outlier_counts(df, threshold):\n",
    "    df = df.copy()\n",
    "    data_numerical = data.select_dtypes(\"number\")\n",
    "\n",
    "    # Get the z-score for specified threshold\n",
    "    threshold_z_score = stats.norm.ppf(threshold)\n",
    "\n",
    "    # Get the z-scores for each value in df\n",
    "    z_score_df = pd.DataFrame(np.abs(stats.zscore(data_numerical)), columns=data_numerical.columns)\n",
    "\n",
    "    # Compare df z_scores to the threshold and return the count of outliers in each column\n",
    "    return (z_score_df > threshold_z_score).sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsKGQMLa2RmS"
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df, threshold):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Get the z-score for specified threshold\n",
    "    threshold_z_score = stats.norm.ppf(threshold)\n",
    "\n",
    "    # Calculate z-scores for numeric columns\n",
    "    z_score_df = pd.DataFrame(np.abs(stats.zscore(numeric_cols, nan_policy='omit')), columns=numeric_cols.columns)\n",
    "    z_score_df = z_score_df > threshold_z_score\n",
    "\n",
    "    # Identify rows with any outliers\n",
    "    outliers = z_score_df.any(axis=1)\n",
    "    outlier_indices = df.index[outliers]\n",
    "\n",
    "    # Drop rows with outliers and reset index without keeping the old index\n",
    "    df = df.drop(outlier_indices, axis=0).reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "02Jvw6ff1RVc",
    "outputId": "c4c6a75d-15ca-4a96-eee1-6cab42c8e198"
   },
   "outputs": [],
   "source": [
    "outlier_threshold = 0.999999\n",
    "get_outlier_counts(data, 0.999999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLA7YZ_V2sJz"
   },
   "outputs": [],
   "source": [
    "data = remove_outliers(data, outlier_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQcb3T_zkgaC",
    "outputId": "b1c073f7-554a-41c3-b614-6114fd145f4f"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhLdsV6XE_ml"
   },
   "source": [
    "Our reduced dataset has 110,885 data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_6xhqm1VpCi"
   },
   "source": [
    "### Perform Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ail_n2hBFECP"
   },
   "source": [
    "Here we perform principal component analysis using 11 principal components, as we determined was optimal in our PCA check-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "WV8xkQj82vY6",
    "outputId": "1947898f-ae80-4914-ded0-6d92a5f22588"
   },
   "outputs": [],
   "source": [
    "data_numerical = data.select_dtypes(\"number\")\n",
    "data_standardized = data_numerical.apply(lambda x: (x - x.mean()) / x.std())\n",
    "pca = PCA(svd_solver=\"full\")\n",
    "pca.fit(data_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5aoSbVm3LkT"
   },
   "outputs": [],
   "source": [
    "n_components = 11\n",
    "pca_final = pca.components_[:, :n_components]\n",
    "pca_df = pd.DataFrame(data=pca_final, columns=[f\"PC {i+1}\" for i in range(n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "4uiBn7FemdXQ",
    "outputId": "2fd646c6-c33d-4910-9dbf-1b2b796a6751"
   },
   "outputs": [],
   "source": [
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqSHKRDBItJZ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qbj1EzCTVuBp"
   },
   "source": [
    "### Generate New Data Frame With Principal Components and Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-beK4zU_FV6A"
   },
   "source": [
    "We verify that there are no null or NaN values within our data frames before we multiply them to have each point alongside its 11 principal components. Additionally, we add the genre back to our principal component data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLDPW86Fuc1t",
    "outputId": "6d722d6b-6c2b-46a4-8e38-62e5607f8f90"
   },
   "outputs": [],
   "source": [
    "print(data_standardized.isna().sum())\n",
    "print(pca_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBkLEXZqsR9V",
    "outputId": "495ff739-0401-472e-983a-2b7a384e5e66"
   },
   "outputs": [],
   "source": [
    "print(data_standardized.shape)\n",
    "print(pca_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "Da0WkBthk2BC",
    "outputId": "02a532b6-8e80-49c7-e01d-8630b103ab39"
   },
   "outputs": [],
   "source": [
    "# Create data_pcs which has our original data in terms of principal components\n",
    "data_pcs = pd.DataFrame(data_standardized.dot(pca_final))\n",
    "print(data_pcs.shape)\n",
    "print(data_pcs.isna().sum())\n",
    "data_pcs[\"track_genre\"] = data[\"track_genre\"]\n",
    "data_pcs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuV7e3_0w62t",
    "outputId": "7e778ce2-ca0a-4ffc-fcea-a7fd206cd377"
   },
   "outputs": [],
   "source": [
    "# list features on data_pcs\n",
    "data_pcs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kysueegJS0km"
   },
   "source": [
    "# CLASSIFICATION - Genre Prediction\n",
    "\n",
    "We will use select columns from the dataset to build a neural network that can ideally predict the genre of a song based on its other features (ie. multi-class classification).\n",
    "\n",
    "We will use only `danceability`, `energy`, `loudness`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`.\n",
    "We believe the other columns are not as relevant to genre prediction, and can be safely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qulSPdT73Bb7"
   },
   "source": [
    "There are 114 genres in this dataset - that is simply too much, and will definitely reduce the accuracy of any model.\n",
    "So, we will extract the most relevant (determined by us) genres.\n",
    "We chose the following 24:\n",
    "- alt-rock\n",
    "- ambient\n",
    "- blues\n",
    "- country\n",
    "- disco\n",
    "- edm\n",
    "- electronic\n",
    "- folk\n",
    "- funk\n",
    "- gospel\n",
    "- grunge\n",
    "- hip-hop\n",
    "- indie\n",
    "- k-pop\n",
    "- latin\n",
    "- metal\n",
    "- pop\n",
    "- punk\n",
    "- r-n-b\n",
    "- reggae\n",
    "- rock\n",
    "- singer-songwriter\n",
    "- soul\n",
    "- techno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn8tXYn9f9CP"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8IcRt3UsybS"
   },
   "source": [
    "#### Standardize data and create genre mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "M6I8Fn4eQ4YL",
    "outputId": "e18a62d2-b32e-4d25-d0b4-bb3d91c47e1a"
   },
   "outputs": [],
   "source": [
    "# only selected genres\n",
    "genres = [\"alt-rock\", \"ambient\", \"blues\", \"country\", \"disco\", \"edm\", \"electronic\", \"folk\", \"funk\", \"gospel\", \"grunge\", \"hip-hop\", \"indie\", \"k-pop\", \"latin\", \"metal\", \"pop\", \"punk\", \"r-n-b\", \"reggae\", \"rock\", \"singer-songwriter\", \"soul\", \"techno\"]\n",
    "data_genres = data_pcs[data_pcs[\"track_genre\"].isin(genres)]\n",
    "\n",
    "# extract relevant features\n",
    "features_c = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "target_c = \"track_genre\"\n",
    "\n",
    "ss_c_x = StandardScaler()\n",
    "ss_c_y = StandardScaler()\n",
    "\n",
    "X_c = data_genres[features_c]\n",
    "y_c = data_genres[target_c]\n",
    "\n",
    "# standardize data\n",
    "X_c = ss_c_x.fit_transform(X_c)\n",
    "X_c = pd.DataFrame(X_c, columns=features_c)\n",
    "\n",
    "\n",
    "# encode labels\n",
    "le = LabelEncoder()\n",
    "y_c = le.fit_transform(y_c)\n",
    "\n",
    "genre_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "\n",
    "X_c.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTLEuV1dqPtZ"
   },
   "source": [
    "#### Create training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3YQoYKETprS"
   },
   "outputs": [],
   "source": [
    "# 80-20 train-test split\n",
    "X_tv_c, X_test_c, y_tv_c, y_test_c = train_test_split(X_c, y_c, test_size=0.2, random_state=8)\n",
    "X_train_c, X_val_c, y_train_c, y_val_c = train_test_split(X_tv_c, y_tv_c, test_size=0.25, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU8L1FykZO6B"
   },
   "outputs": [],
   "source": [
    "# convert to PyTorch tensors\n",
    "X_train_tensor_c = torch.tensor(X_train_c.values, dtype=torch.float32)\n",
    "y_train_tensor_c = torch.tensor(y_train_c, dtype=torch.long)\n",
    "X_val_tensor_c = torch.tensor(X_val_c.values, dtype=torch.float32)\n",
    "y_val_tensor_c = torch.tensor(y_val_c, dtype=torch.long)\n",
    "X_test_tensor_c = torch.tensor(X_test_c.values, dtype=torch.float32)\n",
    "y_test_tensor_c = torch.tensor(y_test_c, dtype=torch.long)\n",
    "\n",
    "# create PyTorch datasets\n",
    "train_dataset_c = TensorDataset(X_train_tensor_c, y_train_tensor_c)\n",
    "val_dataset_c = TensorDataset(X_val_tensor_c, y_val_tensor_c)\n",
    "test_dataset_c = TensorDataset(X_test_tensor_c, y_test_tensor_c)\n",
    "\n",
    "# create DataLoaders for batch processing\n",
    "train_loader_c = DataLoader(train_dataset_c, batch_size=32, shuffle=True)\n",
    "val_loader_c = DataLoader(val_dataset_c, batch_size=32, shuffle=False)\n",
    "test_loader_c = DataLoader(test_dataset_c, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii45Vixyf-eg"
   },
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fld1SWPrbS9V"
   },
   "outputs": [],
   "source": [
    "class GenreClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(GenreClassifier, self).__init__()\n",
    "\n",
    "        # first hidden layer\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        # second hidden layer\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # third hidden layer\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        # fourth hidden layer\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.BatchNorm1d(32)\n",
    "\n",
    "        # output layer\n",
    "        self.output = nn.Linear(32, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRVdrmiiqmFu"
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IXKrDxBqyYj"
   },
   "outputs": [],
   "source": [
    "num_features_c = len(features_c)\n",
    "num_classes_c = len(genre_mapping)\n",
    "learning_rate_c = 0.0005\n",
    "num_epochs_c = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18Bm4I-IqdW3"
   },
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBJRw8eGmVVG",
    "outputId": "ef28b36a-01fa-497d-ae91-fd8f67ed526e"
   },
   "outputs": [],
   "source": [
    "model_c = GenreClassifier(input_size=num_features_c, num_classes=num_classes_c).to(device)\n",
    "model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azs-uY5zj7v4"
   },
   "outputs": [],
   "source": [
    "criterion_c = nn.CrossEntropyLoss()\n",
    "optimizer_c = optim.AdamW(model_c.parameters(), lr=learning_rate_c, weight_decay=1e-5)\n",
    "scheduler_c = optim.lr_scheduler.CosineAnnealingLR(optimizer_c, T_max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKaB63sBqgTn"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Xb5aWf3j-hv",
    "outputId": "f8382fa9-b31b-4cfb-8982-90bb6f02cc4e"
   },
   "outputs": [],
   "source": [
    "def train_model_c(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # TRAINING STAGE\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # forward\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # VALIDATION STAGE\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                X_batch, y_batch = batch\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(predictions, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        # add losses to array\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                f\"Train Loss: {train_loss:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, \"\n",
    "                f\"Val Accuracy: {val_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Step the scheduler based on validation loss\n",
    "        scheduler.step()\n",
    "\n",
    "        # Switch back to training mode for the next epoch\n",
    "        model.train()\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "train_losses, val_losses = train_model_c(model_c, train_loader_c, val_loader_c, criterion_c, optimizer_c, scheduler_c, num_epochs_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "oHjFI5g8WEEI",
    "outputId": "18a818d6-f218-48dc-a861-814d56bc734f"
   },
   "outputs": [],
   "source": [
    "# plotting training and validation loss\n",
    "plt.plot(train_losses, c=\"b\", label='Training Loss')\n",
    "plt.plot(val_losses, c=\"r\", label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYla6IQUsaUo"
   },
   "source": [
    "Based purely off of the loss curves, one could assume that the model would perform well. There are no obvious oscillations, training loss is steadily decreasing while validation loss seems to have plateaued off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-1LxbnEHgvJ"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDFE5gB0Gzo6",
    "outputId": "0ca97b11-d461-4342-b3bc-dd612450ab2f"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model_c(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# top n evaluation function\n",
    "def evaluate_model_top_n(model, test_loader, n=3):\n",
    "    \"\"\"\n",
    "    Evaluates the model accuracy based on top-N predictions.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to evaluate.\n",
    "        test_loader: DataLoader for the test dataset.\n",
    "        n: Number of top predictions to consider for accuracy.\n",
    "\n",
    "    Returns:\n",
    "        Top-N accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Model predictions\n",
    "            predictions = model(X_batch)  # Raw logits\n",
    "            top_probs, top_classes = torch.topk(predictions, n, dim=1)  # Top-N predictions\n",
    "\n",
    "            # Check if the true label is in the top-N predictions\n",
    "            for i in range(len(y_batch)):\n",
    "                if y_batch[i].item() in top_classes[i]:\n",
    "                    correct += 1\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    # Calculate Top-N accuracy\n",
    "    top_n_accuracy = correct / total\n",
    "    print(f\"Top-{n} Accuracy: {top_n_accuracy * 100:.2f}%\")\n",
    "# Evaluate the model\n",
    "evaluate_model_c(model_c, test_loader_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OA7zfztHwem",
    "outputId": "032ef382-c089-49be-d990-287774fc0de1"
   },
   "outputs": [],
   "source": [
    "evaluate_model_c(model_c, test_loader_c)\n",
    "evaluate_model_top_n(model_c, test_loader_c, 2)\n",
    "evaluate_model_top_n(model_c, test_loader_c, 3)\n",
    "evaluate_model_top_n(model_c, test_loader_c, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 671
    },
    "id": "XZVYE9aYmqaI",
    "outputId": "2c3d536a-0da0-4d96-8c34-209cbffcb2ef"
   },
   "outputs": [],
   "source": [
    "# generate confusion matrix\n",
    "def generate_confusion_matrix(model, test_loader, num_classes, genre_names):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Convert predictions and targets to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genre_names, yticklabels=genre_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "generate_confusion_matrix(model_c, test_loader_c, num_classes_c, genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDpkwZyvOnow"
   },
   "source": [
    "From the diagonal observations in the plot, we observe that the model performed really unevenly at classification for each class. For example, Ambient and Techno have a large number of observation correctly predicted (probably because their music is very distinct compared to the other genres) whereas alt-roc, folk and indie have few or no correctly predicted observation (probably because their music can be reasonably categorized as other genres)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
